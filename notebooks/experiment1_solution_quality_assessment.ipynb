{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c5a8c06-ed7b-44e4-9945-0d2b78a564cc",
   "metadata": {},
   "source": [
    "# Experiment 1: DP vs. SAA Solution Quality Assessment\n",
    "## Full Scale Implementation: Key Components and Design Decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ac013e-57bd-439c-8e05-7e4d679e747a",
   "metadata": {},
   "source": [
    "1. **Experiment Structure**\n",
    "The implementation uses a class-based approach with `Experiment1Runner` to encapsulate all experiment functionality. This provides clear organization and makes the code maintainable and extensible.\n",
    "\n",
    "2. **Test Instance Generation**\n",
    "The code generates test instances using the provided `data_generator.py` script, varying:\n",
    "- Capacity levels (3, 5, 7 rooms)\n",
    "- Demand scenarios (low, base, high)\n",
    "- Market conditions (budget, standard, luxury)\n",
    "\n",
    "3. **Parallel Processing**\n",
    "The implementation uses Python's `ProcessPoolExecutor` to run experiments in parallel, significantly reducing execution time for the 30 replications per configuration.\n",
    "\n",
    "4. **Statistical Analysis**\n",
    "The code performs comprehensive statistical analysis including:\n",
    "- Paired t-tests comparing SAA and DP revenues\n",
    "- 95% confidence intervals for revenue gaps\n",
    "- Analysis by capacity level and demand scenario\n",
    "\n",
    "5. **Visualization and Reporting**\n",
    "The implementation creates three key visualizations:\n",
    "- Revenue comparison bar charts\n",
    "- Revenue gap box plots\n",
    "- Solution time comparisons\n",
    "\n",
    "6. **Results Management**\n",
    "All results are saved systematically:\n",
    "- Raw data in CSV format\n",
    "- Analysis results in JSON format\n",
    "- Visualizations as PNG files\n",
    "- Comprehensive report in Markdown format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b127d9-bbe6-4ad9-862b-d13019e414fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: DP vs. SAA Solution Quality Assessment\n",
    "# Full Scale Implementation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "from data_generator import TestConfiguration, create_test_instance\n",
    "from dynamic_pricing_algorithms import DynamicProgramming, StochasticApproximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf8c29d-2f5c-44c4-9187-f59fe3cfda38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5524dd6-0e81-4298-b269-a5cf53b3ac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment1Runner:\n",
    "    \"\"\"\n",
    "    Runner class for Experiment 1: Solution Quality Assessment\n",
    "    Compares SAA performance against optimal DP solution for small instances.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_results_dir: str = \"../experiments/experiment1\"):\n",
    "        \"\"\"\n",
    "        Initialize experiment runner with configuration.\n",
    "        \n",
    "        Args:\n",
    "            base_results_dir: Path to the base results directory, relative to the script location\n",
    "        \"\"\"\n",
    "        # Get the directory where the script is located\n",
    "        script_dir = Path(__file__).parent.absolute()\n",
    "        \n",
    "        # Construct the full path to the results directory\n",
    "        self.base_results_dir = Path(script_dir) / base_results_dir\n",
    "        self.output_dir = self.base_results_dir / \"results\"\n",
    "        \n",
    "        # Create the experiment directory if it doesn't exist\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Log the results directory location\n",
    "        logger.info(f\"Results will be saved to: {self.output_dir}\")\n",
    "        \n",
    "        # Define experiment parameters\n",
    "        self.T = 5  # Booking horizon\n",
    "        self.N = 3  # Service horizon\n",
    "        \n",
    "        # Define parameter ranges for test instances\n",
    "        # self.capacity_levels = [3, 5, 7]  # Small capacities for tractable DP\n",
    "        # self.demand_scenarios = ['low', 'base', 'high']\n",
    "        # self.market_conditions = ['budget', 'standard', 'luxury']\n",
    "        \n",
    "        self.capacity_levels = [3]  # Small capacities for tractable DP\n",
    "        self.demand_scenarios = ['base']\n",
    "        self.market_conditions = ['budget']\n",
    "        \n",
    "        # SAA learning parameters\n",
    "        self.learning_params = {\n",
    "            'eta_0': 0.5,        # Initial learning rate\n",
    "            'gamma': 0.05,       # Learning rate decay\n",
    "            'eta_min': 0.001,    # Minimum learning rate\n",
    "            'max_epochs': 1000,  # Maximum training epochs\n",
    "            'batch_size': 64     # Mini-batch size\n",
    "        }\n",
    "        \n",
    "        # Statistical parameters\n",
    "        self.num_replications = 30  # Number of replications per configuration\n",
    "        self.confidence_level = 0.95\n",
    "        \n",
    "    def generate_test_instance(self, \n",
    "                             capacity: int,\n",
    "                             demand_scenario: str,\n",
    "                             market_condition: str,\n",
    "                             seed: int) -> Dict:\n",
    "        \"\"\"Generate a single test instance with specified parameters.\"\"\"\n",
    "        # Configure test parameters\n",
    "        config = TestConfiguration()\n",
    "        test_params = config.get_config(\n",
    "            test_type='minimal',\n",
    "            market_condition=market_condition,\n",
    "            discretization='standard'\n",
    "        )\n",
    "        \n",
    "        # Override with experiment-specific parameters\n",
    "        test_params.update({\n",
    "            'T': self.T,\n",
    "            'N': self.N,\n",
    "            'C': capacity\n",
    "        })\n",
    "        \n",
    "        # Create and return test instance\n",
    "        return create_test_instance(\n",
    "            demand_scenario=demand_scenario,\n",
    "            market_condition=market_condition,\n",
    "            test_configuration=test_params,\n",
    "            seed=seed\n",
    "        )\n",
    "    \n",
    "    def run_single_instance(self,\n",
    "                          instance: Dict,\n",
    "                          replication: int,\n",
    "                          eval_seed: int = None) -> Dict:\n",
    "        \"\"\"Run both DP and SAA on a single test instance with shared sample paths.\n",
    "        \n",
    "        Args:\n",
    "            instance: Test instance dictionary\n",
    "            replication: Replication number\n",
    "            eval_seed: Seed for generating evaluation paths (ensures consistency)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Initialize SAA first to generate sample paths\n",
    "            saa = StochasticApproximation(instance, self.learning_params)\n",
    "            \n",
    "            # Set evaluation seed for consistent path generation\n",
    "            eval_rng = np.random.default_rng(eval_seed if eval_seed is not None \n",
    "                                           else 1000 * replication)\n",
    "            \n",
    "            # Generate fixed sample paths for evaluation using controlled RNG\n",
    "            num_eval_paths = 1000\n",
    "            evaluation_paths = []\n",
    "            for _ in range(num_eval_paths):\n",
    "                path = saa._generate_sample_path(rng=eval_rng)\n",
    "                evaluation_paths.append(path)\n",
    "                \n",
    "            # Validate evaluation paths\n",
    "            self._validate_evaluation_paths(evaluation_paths, instance)\n",
    "            \n",
    "            # Solve using Dynamic Programming\n",
    "            dp = DynamicProgramming(instance)\n",
    "            dp_start = datetime.now()\n",
    "            dp_policy, _ = dp.solve()\n",
    "            dp_time = (datetime.now() - dp_start).total_seconds()\n",
    "            \n",
    "            # Evaluate DP policy on the fixed sample paths\n",
    "            dp_revenue = dp.evaluate_policy(dp_policy, evaluation_paths)\n",
    "            \n",
    "            # Solve using SAA\n",
    "            saa_start = datetime.now()\n",
    "            saa_prices, _, saa_time = saa.solve()\n",
    "            \n",
    "            # Evaluate SAA policy on the same fixed sample paths\n",
    "            saa_revenue = saa.evaluate(saa_prices, evaluation_paths)\n",
    "            \n",
    "            # Compute revenue gap\n",
    "            revenue_gap = ((dp_revenue - saa_revenue) / dp_revenue) * 100\n",
    "            \n",
    "            return {\n",
    "                'capacity': instance['parameters'].C,\n",
    "                'demand_scenario': instance['scenario_info']['demand_scenario'],\n",
    "                'market_condition': instance['scenario_info']['market_condition'],\n",
    "                'replication': replication,\n",
    "                'dp_revenue': dp_revenue,\n",
    "                'dp_time': dp_time,\n",
    "                'saa_revenue': saa_revenue,\n",
    "                'saa_time': saa_time,\n",
    "                'revenue_gap': revenue_gap,\n",
    "                'num_eval_paths': num_eval_paths,\n",
    "                'eval_seed': eval_seed if eval_seed is not None else 1000 * replication\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing instance: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _validate_evaluation_paths(self, paths: List, instance: Dict):\n",
    "        \"\"\"Validate evaluation paths for consistency with instance parameters.\"\"\"\n",
    "        if not paths:\n",
    "            raise ValueError(\"Empty evaluation paths\")\n",
    "            \n",
    "        # Check path length matches booking horizon\n",
    "        expected_length = instance['parameters'].T\n",
    "        if any(len(path) != expected_length for path in paths):\n",
    "            raise ValueError(\"Inconsistent path lengths\")\n",
    "            \n",
    "        # Verify booking classes are valid\n",
    "        valid_classes = set(instance['booking_classes'])\n",
    "        for path in paths:\n",
    "            for _, bt, _ in path:\n",
    "                if bt is not None and bt not in valid_classes:\n",
    "                    raise ValueError(f\"Invalid booking class {bt}\")\n",
    "                    \n",
    "        logger.info(\"Evaluation paths validated successfully\")\n",
    "        \n",
    "    def run_experiment(self, num_workers: int = 4) -> pd.DataFrame:\n",
    "        \"\"\"Run the complete experiment with all parameter combinations.\"\"\"\n",
    "        logger.info(\"Starting Experiment 1: Solution Quality Assessment\")\n",
    "        \n",
    "        # Generate parameter combinations\n",
    "        combinations = list(itertools.product(\n",
    "            self.capacity_levels,\n",
    "            self.demand_scenarios,\n",
    "            self.market_conditions,\n",
    "            range(self.num_replications)\n",
    "        ))\n",
    "        \n",
    "        # Initialize results storage\n",
    "        results = []\n",
    "        \n",
    "        # Run experiments in parallel\n",
    "        with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "            future_to_params = {\n",
    "                executor.submit(\n",
    "                    self.run_single_instance,\n",
    "                    self.generate_test_instance(\n",
    "                        capacity=c,\n",
    "                        demand_scenario=d,\n",
    "                        market_condition=m,\n",
    "                        seed=100*r + 1\n",
    "                    ),\n",
    "                    r\n",
    "                ): (c, d, m, r) for c, d, m, r in combinations\n",
    "            }\n",
    "            \n",
    "            for future in future_to_params:\n",
    "                result = future.result()\n",
    "                if result is not None:\n",
    "                    results.append(result)\n",
    "        \n",
    "        # Convert results to DataFrame\n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        # Save raw results\n",
    "        results_df.to_csv(self.output_dir / 'raw_results.csv', index=False)\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def analyze_results(self, results_df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Perform statistical analysis on experiment results.\"\"\"\n",
    "        analysis = {}\n",
    "\n",
    "        # Overall statistics\n",
    "        analysis['overall'] = {\n",
    "            'mean_revenue_gap': float(results_df['revenue_gap'].mean()),  # Convert to float\n",
    "            'std_revenue_gap': float(results_df['revenue_gap'].std()),\n",
    "            'mean_dp_time': float(results_df['dp_time'].mean()),\n",
    "            'mean_saa_time': float(results_df['saa_time'].mean())\n",
    "        }\n",
    "\n",
    "        # Paired t-test for revenue differences\n",
    "        t_stat, p_value = stats.ttest_rel(\n",
    "            results_df['dp_revenue'],\n",
    "            results_df['saa_revenue']\n",
    "        )\n",
    "\n",
    "        analysis['statistical_tests'] = {\n",
    "            't_statistic': float(t_stat),  # Convert to float\n",
    "            'p_value': float(p_value)\n",
    "        }\n",
    "\n",
    "        # Confidence intervals for revenue gap\n",
    "        ci = stats.t.interval(\n",
    "            self.confidence_level,\n",
    "            len(results_df) - 1,\n",
    "            loc=results_df['revenue_gap'].mean(),\n",
    "            scale=stats.sem(results_df['revenue_gap'])\n",
    "        )\n",
    "\n",
    "        analysis['confidence_intervals'] = {\n",
    "            'revenue_gap_lower': float(ci[0]),  # Convert to float\n",
    "            'revenue_gap_upper': float(ci[1])\n",
    "        }\n",
    "\n",
    "        # Analysis by capacity level\n",
    "        capacity_analysis = results_df.groupby('capacity').agg({\n",
    "            'revenue_gap': ['mean', 'std'],\n",
    "            'dp_time': 'mean',\n",
    "            'saa_time': 'mean'\n",
    "        })\n",
    "\n",
    "        # Convert the nested dictionary structure to a more JSON-serializable format\n",
    "        analysis['by_capacity'] = {\n",
    "            str(cap): {  # Convert capacity to string\n",
    "                'revenue_gap_mean': float(stats['revenue_gap']['mean']),\n",
    "                'revenue_gap_std': float(stats['revenue_gap']['std']),\n",
    "                'dp_time_mean': float(stats['dp_time']['mean']),\n",
    "                'saa_time_mean': float(stats['saa_time']['mean'])\n",
    "            }\n",
    "            for cap, stats in capacity_analysis.iterrows()\n",
    "        }\n",
    "\n",
    "        # Save analysis results\n",
    "        with open(self.output_dir / 'analysis_results.json', 'w') as f:\n",
    "            json.dump(analysis, f, indent=4)\n",
    "\n",
    "        return analysis\n",
    "\n",
    "    def generate_report(self, results_df: pd.DataFrame, analysis: Dict):\n",
    "        \"\"\"Generate a comprehensive report of experimental results.\"\"\"\n",
    "        try:\n",
    "            report = []\n",
    "            report.append(\"# Experiment 1: Solution Quality Assessment Report\")\n",
    "            report.append(\"\\n## Overview\")\n",
    "            report.append(f\"- Total test instances: {len(results_df)}\")\n",
    "            report.append(f\"- Capacity levels: {sorted(results_df['capacity'].unique())}\")\n",
    "            report.append(f\"- Demand scenarios: {sorted(results_df['demand_scenario'].unique())}\")\n",
    "            report.append(f\"- Market conditions: {sorted(results_df['market_condition'].unique())}\")\n",
    "            report.append(f\"- Replications per configuration: {self.num_replications}\")\n",
    "\n",
    "            report.append(\"\\n## Overall Results\")\n",
    "            report.append(f\"- Mean revenue gap: {analysis['overall']['mean_revenue_gap']:.2f}%\")\n",
    "            report.append(f\"- Revenue gap 95% CI: [{analysis['confidence_intervals']['revenue_gap_lower']:.2f}%, \"\n",
    "                         f\"{analysis['confidence_intervals']['revenue_gap_upper']:.2f}%]\")\n",
    "            report.append(f\"- Mean DP solution time: {analysis['overall']['mean_dp_time']:.2f} seconds\")\n",
    "            report.append(f\"- Mean SAA solution time: {analysis['overall']['mean_saa_time']:.2f} seconds\")\n",
    "\n",
    "            report.append(\"\\n## Statistical Analysis\")\n",
    "            report.append(f\"- T-statistic: {analysis['statistical_tests']['t_statistic']:.4f}\")\n",
    "            report.append(f\"- P-value: {analysis['statistical_tests']['p_value']:.4f}\")\n",
    "\n",
    "            report.append(\"\\n## Results by Capacity Level\")\n",
    "            for capacity in sorted(results_df['capacity'].unique()):\n",
    "                cap_stats = analysis['by_capacity'][str(capacity)]\n",
    "                report.append(f\"\\nCapacity = {capacity}\")\n",
    "                report.append(f\"- Mean revenue gap: {cap_stats['revenue_gap_mean']:.2f}%\")\n",
    "                report.append(f\"- Revenue gap std: {cap_stats['revenue_gap_std']:.2f}%\")\n",
    "                report.append(f\"- Mean DP time: {cap_stats['dp_time_mean']:.2f} seconds\")\n",
    "                report.append(f\"- Mean SAA time: {cap_stats['saa_time_mean']:.2f} seconds\")\n",
    "\n",
    "            # Add summary statistics by demand scenario\n",
    "            report.append(\"\\n## Results by Demand Scenario\")\n",
    "            demand_stats = results_df.groupby('demand_scenario').agg({\n",
    "                'revenue_gap': ['mean', 'std'],\n",
    "                'dp_time': 'mean',\n",
    "                'saa_time': 'mean'\n",
    "            })\n",
    "\n",
    "            for scenario in sorted(results_df['demand_scenario'].unique()):\n",
    "                stats = demand_stats.loc[scenario]\n",
    "                report.append(f\"\\nScenario: {scenario}\")\n",
    "                report.append(f\"- Mean revenue gap: {stats['revenue_gap']['mean']:.2f}%\")\n",
    "                report.append(f\"- Revenue gap std: {stats['revenue_gap']['std']:.2f}%\")\n",
    "                report.append(f\"- Mean DP time: {stats['dp_time']['mean']:.2f} seconds\")\n",
    "                report.append(f\"- Mean SAA time: {stats['saa_time']['mean']:.2f} seconds\")\n",
    "\n",
    "            # Add summary statistics by market condition\n",
    "            report.append(\"\\n## Results by Market Condition\")\n",
    "            market_stats = results_df.groupby('market_condition').agg({\n",
    "                'revenue_gap': ['mean', 'std'],\n",
    "                'dp_time': 'mean',\n",
    "                'saa_time': 'mean'\n",
    "            })\n",
    "\n",
    "            for market in sorted(results_df['market_condition'].unique()):\n",
    "                stats = market_stats.loc[market]\n",
    "                report.append(f\"\\nMarket: {market}\")\n",
    "                report.append(f\"- Mean revenue gap: {stats['revenue_gap']['mean']:.2f}%\")\n",
    "                report.append(f\"- Revenue gap std: {stats['revenue_gap']['std']:.2f}%\")\n",
    "                report.append(f\"- Mean DP time: {stats['dp_time']['mean']:.2f} seconds\")\n",
    "                report.append(f\"- Mean SAA time: {stats['saa_time']['mean']:.2f} seconds\")\n",
    "\n",
    "            # Save report\n",
    "            with open(self.output_dir / 'experiment_report.md', 'w') as f:\n",
    "                f.write('\\n'.join(report))\n",
    "\n",
    "            logger.info(\"Report generated successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating report: {str(e)}\")\n",
    "            logger.error(\"Results DataFrame head:\")\n",
    "            logger.error(results_df.head())\n",
    "            logger.error(\"Analysis structure:\")\n",
    "            logger.error(json.dumps(analysis, indent=2))\n",
    "            raise\n",
    "            \n",
    "    def create_visualizations(self, results_df: pd.DataFrame):\n",
    "        \"\"\"Create and save visualizations of experimental results.\"\"\"\n",
    "        import matplotlib\n",
    "        matplotlib.use('Agg')  # Set backend to non-interactive\n",
    "\n",
    "        # Reset any existing plots\n",
    "        plt.close('all')\n",
    "\n",
    "        try:\n",
    "            # 1. Revenue Comparison Visualization\n",
    "            fig1, ax1 = plt.subplots(figsize=(12, 6))\n",
    "            revenue_plot = sns.barplot(\n",
    "                data=results_df,\n",
    "                x='capacity',\n",
    "                y='dp_revenue',\n",
    "                hue='demand_scenario',\n",
    "                errorbar=('ci', 95),\n",
    "                ax=ax1\n",
    "            )\n",
    "            ax1.set_title('Dynamic Programming Revenue by Capacity and Demand Scenario')\n",
    "            ax1.set_xlabel('Capacity Level')\n",
    "            ax1.set_ylabel('Revenue')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(self.output_dir / 'revenue_comparison.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close(fig1)\n",
    "            logger.info(\"Generated revenue comparison plot\")\n",
    "\n",
    "            # 2. Revenue Gap Distribution\n",
    "            fig2, ax2 = plt.subplots(figsize=(12, 6))\n",
    "            gap_plot = sns.boxplot(\n",
    "                data=results_df,\n",
    "                x='capacity',\n",
    "                y='revenue_gap',\n",
    "                hue='market_condition',\n",
    "                ax=ax2\n",
    "            )\n",
    "            ax2.set_title('Revenue Gap Distribution by Capacity and Market Condition')\n",
    "            ax2.set_xlabel('Capacity Level')\n",
    "            ax2.set_ylabel('Revenue Gap (%)')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(self.output_dir / 'revenue_gap_distribution.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close(fig2)\n",
    "            logger.info(\"Generated revenue gap distribution plot\")\n",
    "\n",
    "            # 3. Solution Time Comparison\n",
    "            fig3, ax3 = plt.subplots(figsize=(12, 6))\n",
    "            time_data = pd.melt(\n",
    "                results_df,\n",
    "                id_vars=['capacity'],\n",
    "                value_vars=['dp_time', 'saa_time'],\n",
    "                var_name='Algorithm',\n",
    "                value_name='Time (seconds)'\n",
    "            )\n",
    "            time_plot = sns.boxplot(\n",
    "                data=time_data,\n",
    "                x='capacity',\n",
    "                y='Time (seconds)',\n",
    "                hue='Algorithm',\n",
    "                ax=ax3\n",
    "            )\n",
    "            ax3.set_title('Solution Time Comparison by Algorithm')\n",
    "            ax3.set_xlabel('Capacity Level')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(self.output_dir / 'solution_time_comparison.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close(fig3)\n",
    "            logger.info(\"Generated solution time comparison plot\")\n",
    "\n",
    "            logger.info(\"Successfully created all visualizations\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating visualizations: {str(e)}\")\n",
    "            # Print additional debugging information\n",
    "            logger.error(\"Matplotlib version: \" + matplotlib.__version__)\n",
    "            logger.error(\"Seaborn version: \" + sns.__version__)\n",
    "            raise\n",
    "\n",
    "        finally:\n",
    "            # Ensure all plots are closed\n",
    "            plt.close('all')\n",
    "    \n",
    "    def run_full_experiment(self, num_workers: int = 4):\n",
    "        \"\"\"Execute the complete experiment workflow.\"\"\"\n",
    "        logger.info(\"Starting full experiment execution\")\n",
    "\n",
    "        try:\n",
    "            # Run experiments\n",
    "            results_df = self.run_experiment(num_workers)\n",
    "            logger.info(\"Experiments completed successfully\")\n",
    "\n",
    "            # Analyze results\n",
    "            analysis = self.analyze_results(results_df)\n",
    "            logger.info(\"Analysis completed successfully\")\n",
    "\n",
    "            # Generate report\n",
    "            self.generate_report(results_df, analysis)\n",
    "            logger.info(\"Report generation completed successfully\")\n",
    "\n",
    "            # Create visualizations\n",
    "            self.create_visualizations(results_df)\n",
    "            logger.info(\"Visualization creation completed successfully\")\n",
    "\n",
    "            logger.info(\"Experiment execution completed\")\n",
    "            return results_df, analysis\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in experiment execution: {str(e)}\")\n",
    "            raise\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b29e848-6861-408b-baad-d1fe6dbe61af",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Add process safety for macOS\n",
    "    import multiprocessing\n",
    "    multiprocessing.set_start_method('spawn')\n",
    "    \n",
    "    # Run the complete experiment\n",
    "    experiment = Experiment1Runner()\n",
    "    try:\n",
    "        results, analysis = experiment.run_full_experiment(num_workers=8)\n",
    "        print(\"Experiment completed successfully\")\n",
    "        print(f\"Results saved to: {experiment.output_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error running experiment: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b078c06-24cd-48a8-9043-28908806b636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618efd4b-1d02-419a-9f55-19c85a3efd47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ade052c-98d3-453a-ae5d-fbe260ad8afb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32faeab5-c88e-4daf-995a-a7aaae5029f1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc28556d-4611-45d9-95e8-4b195c2c316a",
   "metadata": {},
   "source": [
    "# Experiment 1: Solution Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eb596a-f4bb-4d87-843f-c96d73f73431",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment1_solution_quality_assessment_full import Experiment1Runner\n",
    "# Load the saved results\n",
    "results_df = pd.read_csv('results/experiment1/raw_results.csv')\n",
    "\n",
    "# Create an experiment instance\n",
    "experiment = Experiment1Runner()\n",
    "\n",
    "# Generate visualizations\n",
    "experiment.create_visualizations(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90ed358-76d7-4a6b-ba79-442d1600bcbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
