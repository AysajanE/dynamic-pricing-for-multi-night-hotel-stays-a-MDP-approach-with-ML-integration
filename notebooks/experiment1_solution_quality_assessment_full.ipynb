{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c5a8c06-ed7b-44e4-9945-0d2b78a564cc",
   "metadata": {},
   "source": [
    "# Experiment 1: DP vs. SAA Solution Quality Assessment\n",
    "## Full Scale Implementation: Key Components and Design Decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ac013e-57bd-439c-8e05-7e4d679e747a",
   "metadata": {},
   "source": [
    "1. **Experiment Structure**\n",
    "The implementation uses a class-based approach with `Experiment1Runner` to encapsulate all experiment functionality. This provides clear organization and makes the code maintainable and extensible.\n",
    "\n",
    "2. **Test Instance Generation**\n",
    "The code generates test instances using the provided `data_generator.py` script, varying:\n",
    "- Capacity levels (3, 5, 7 rooms)\n",
    "- Demand scenarios (low, base, high)\n",
    "- Market conditions (budget, standard, luxury)\n",
    "\n",
    "3. **Parallel Processing**\n",
    "The implementation uses Python's `ProcessPoolExecutor` to run experiments in parallel, significantly reducing execution time for the 30 replications per configuration.\n",
    "\n",
    "4. **Statistical Analysis**\n",
    "The code performs comprehensive statistical analysis including:\n",
    "- Paired t-tests comparing SAA and DP revenues\n",
    "- 95% confidence intervals for revenue gaps\n",
    "- Analysis by capacity level and demand scenario\n",
    "\n",
    "5. **Visualization and Reporting**\n",
    "The implementation creates three key visualizations:\n",
    "- Revenue comparison bar charts\n",
    "- Revenue gap box plots\n",
    "- Solution time comparisons\n",
    "\n",
    "6. **Results Management**\n",
    "All results are saved systematically:\n",
    "- Raw data in CSV format\n",
    "- Analysis results in JSON format\n",
    "- Visualizations as PNG files\n",
    "- Comprehensive report in Markdown format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4ca5fb6-dd2c-4a9d-b026-22c5c89a1063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import itertools\n",
    "\n",
    "from data_generator import TestConfiguration, create_test_instance\n",
    "from dynamic_pricing_algorithms import DynamicProgramming, StochasticApproximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a37d726-4dc8-4071-b6ae-fb84990d35e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382699f5-5411-4165-af56-6298f3150c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment1Runner:\n",
    "    \"\"\"\n",
    "    Runner class for Experiment 1: Solution Quality Assessment\n",
    "    Compares SAA performance against optimal DP solution for small instances.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: str = \"results/experiment1\"):\n",
    "        \"\"\"Initialize experiment runner with configuration.\"\"\"\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Define experiment parameters\n",
    "        self.T = 5  # Booking horizon\n",
    "        self.N = 3  # Service horizon\n",
    "        \n",
    "        # Define parameter ranges for test instances\n",
    "        self.capacity_levels = [3, 5, 7]  # Small capacities for tractable DP\n",
    "        self.demand_scenarios = ['low', 'base', 'high']\n",
    "        self.market_conditions = ['budget', 'standard', 'luxury']\n",
    "        \n",
    "        # SAA learning parameters\n",
    "        self.learning_params = {\n",
    "            'eta_0': 0.5,        # Initial learning rate\n",
    "            'gamma': 0.05,       # Learning rate decay\n",
    "            'eta_min': 0.001,    # Minimum learning rate\n",
    "            'max_epochs': 1000,  # Maximum training epochs\n",
    "            'batch_size': 64     # Mini-batch size\n",
    "        }\n",
    "        \n",
    "        # Statistical parameters\n",
    "        self.num_replications = 30  # Number of replications per configuration\n",
    "        self.confidence_level = 0.95\n",
    "        \n",
    "    def generate_test_instance(self, \n",
    "                             capacity: int,\n",
    "                             demand_scenario: str,\n",
    "                             market_condition: str,\n",
    "                             seed: int) -> Dict:\n",
    "        \"\"\"Generate a single test instance with specified parameters.\"\"\"\n",
    "        # Configure test parameters\n",
    "        config = TestConfiguration()\n",
    "        test_params = config.get_config(\n",
    "            test_type='minimal',\n",
    "            market_condition=market_condition,\n",
    "            discretization='standard'\n",
    "        )\n",
    "        \n",
    "        # Override with experiment-specific parameters\n",
    "        test_params.update({\n",
    "            'T': self.T,\n",
    "            'N': self.N,\n",
    "            'C': capacity\n",
    "        })\n",
    "        \n",
    "        # Create and return test instance\n",
    "        return create_test_instance(\n",
    "            demand_scenario=demand_scenario,\n",
    "            market_condition=market_condition,\n",
    "            test_configuration=test_params,\n",
    "            seed=seed\n",
    "        )\n",
    "    \n",
    "    def run_single_instance(self,\n",
    "                          instance: Dict,\n",
    "                          replication: int) -> Dict:\n",
    "        \"\"\"Run both DP and SAA on a single test instance.\"\"\"\n",
    "        try:\n",
    "            # Solve using Dynamic Programming\n",
    "            dp = DynamicProgramming(instance)\n",
    "            dp_start = datetime.now()\n",
    "            _, dp_revenue = dp.solve()\n",
    "            dp_time = (datetime.now() - dp_start).total_seconds()\n",
    "            \n",
    "            # Solve using SAA\n",
    "            saa = StochasticApproximation(instance, self.learning_params)\n",
    "            saa_start = datetime.now()\n",
    "            prices, saa_revenue, saa_time = saa.solve()\n",
    "            \n",
    "            # Compute revenue gap\n",
    "            revenue_gap = ((dp_revenue - saa_revenue) / dp_revenue) * 100\n",
    "            \n",
    "            return {\n",
    "                'capacity': instance['parameters'].C,\n",
    "                'demand_scenario': instance['scenario_info']['demand_scenario'],\n",
    "                'market_condition': instance['scenario_info']['market_condition'],\n",
    "                'replication': replication,\n",
    "                'dp_revenue': dp_revenue,\n",
    "                'dp_time': dp_time,\n",
    "                'saa_revenue': saa_revenue,\n",
    "                'saa_time': saa_time,\n",
    "                'revenue_gap': revenue_gap\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing instance: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def run_experiment(self, num_workers: int = 4) -> pd.DataFrame:\n",
    "        \"\"\"Run the complete experiment with all parameter combinations.\"\"\"\n",
    "        logger.info(\"Starting Experiment 1: Solution Quality Assessment\")\n",
    "        \n",
    "        # Generate parameter combinations\n",
    "        combinations = list(itertools.product(\n",
    "            self.capacity_levels,\n",
    "            self.demand_scenarios,\n",
    "            self.market_conditions,\n",
    "            range(self.num_replications)\n",
    "        ))\n",
    "        \n",
    "        # Initialize results storage\n",
    "        results = []\n",
    "        \n",
    "        # Run experiments in parallel\n",
    "        with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "            future_to_params = {\n",
    "                executor.submit(\n",
    "                    self.run_single_instance,\n",
    "                    self.generate_test_instance(\n",
    "                        capacity=c,\n",
    "                        demand_scenario=d,\n",
    "                        market_condition=m,\n",
    "                        seed=100*r + 1\n",
    "                    ),\n",
    "                    r\n",
    "                ): (c, d, m, r) for c, d, m, r in combinations\n",
    "            }\n",
    "            \n",
    "            for future in future_to_params:\n",
    "                result = future.result()\n",
    "                if result is not None:\n",
    "                    results.append(result)\n",
    "        \n",
    "        # Convert results to DataFrame\n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        # Save raw results\n",
    "        results_df.to_csv(self.output_dir / 'raw_results.csv', index=False)\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def analyze_results(self, results_df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Perform statistical analysis on experiment results.\"\"\"\n",
    "        analysis = {}\n",
    "\n",
    "        # Overall statistics\n",
    "        analysis['overall'] = {\n",
    "            'mean_revenue_gap': float(results_df['revenue_gap'].mean()),  # Convert to float\n",
    "            'std_revenue_gap': float(results_df['revenue_gap'].std()),\n",
    "            'mean_dp_time': float(results_df['dp_time'].mean()),\n",
    "            'mean_saa_time': float(results_df['saa_time'].mean())\n",
    "        }\n",
    "\n",
    "        # Paired t-test for revenue differences\n",
    "        t_stat, p_value = stats.ttest_rel(\n",
    "            results_df['dp_revenue'],\n",
    "            results_df['saa_revenue']\n",
    "        )\n",
    "\n",
    "        analysis['statistical_tests'] = {\n",
    "            't_statistic': float(t_stat),  # Convert to float\n",
    "            'p_value': float(p_value)\n",
    "        }\n",
    "\n",
    "        # Confidence intervals for revenue gap\n",
    "        ci = stats.t.interval(\n",
    "            self.confidence_level,\n",
    "            len(results_df) - 1,\n",
    "            loc=results_df['revenue_gap'].mean(),\n",
    "            scale=stats.sem(results_df['revenue_gap'])\n",
    "        )\n",
    "\n",
    "        analysis['confidence_intervals'] = {\n",
    "            'revenue_gap_lower': float(ci[0]),  # Convert to float\n",
    "            'revenue_gap_upper': float(ci[1])\n",
    "        }\n",
    "\n",
    "        # Analysis by capacity level\n",
    "        capacity_analysis = results_df.groupby('capacity').agg({\n",
    "            'revenue_gap': ['mean', 'std'],\n",
    "            'dp_time': 'mean',\n",
    "            'saa_time': 'mean'\n",
    "        })\n",
    "\n",
    "        # Convert the nested dictionary structure to a more JSON-serializable format\n",
    "        analysis['by_capacity'] = {\n",
    "            str(cap): {  # Convert capacity to string\n",
    "                'revenue_gap_mean': float(stats['revenue_gap']['mean']),\n",
    "                'revenue_gap_std': float(stats['revenue_gap']['std']),\n",
    "                'dp_time_mean': float(stats['dp_time']['mean']),\n",
    "                'saa_time_mean': float(stats['saa_time']['mean'])\n",
    "            }\n",
    "            for cap, stats in capacity_analysis.iterrows()\n",
    "        }\n",
    "\n",
    "        # Save analysis results\n",
    "        with open(self.output_dir / 'analysis_results.json', 'w') as f:\n",
    "            json.dump(analysis, f, indent=4)\n",
    "\n",
    "        return analysis\n",
    "\n",
    "    def create_visualizations(self, results_df: pd.DataFrame):\n",
    "        \"\"\"Create and save visualizations of experimental results.\"\"\"\n",
    "        # Set style and figure size\n",
    "        plt.style.use('seaborn')\n",
    "        plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "        try:\n",
    "            # 1. Revenue Comparison Bar Chart\n",
    "            plt.figure()\n",
    "            sns.barplot(data=results_df, x='capacity', y='dp_revenue', \n",
    "                       hue='demand_scenario', ci=95)\n",
    "            plt.title('DP Revenue by Capacity and Demand Scenario')\n",
    "            plt.xlabel('Capacity Level')\n",
    "            plt.ylabel('Revenue')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(self.output_dir / 'revenue_comparison.png')\n",
    "            plt.close()\n",
    "\n",
    "            # 2. Revenue Gap Box Plot\n",
    "            plt.figure()\n",
    "            sns.boxplot(data=results_df, x='capacity', y='revenue_gap',\n",
    "                       hue='market_condition')\n",
    "            plt.title('Revenue Gap Distribution by Capacity and Market Condition')\n",
    "            plt.xlabel('Capacity Level')\n",
    "            plt.ylabel('Revenue Gap (%)')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(self.output_dir / 'revenue_gap_distribution.png')\n",
    "            plt.close()\n",
    "\n",
    "            # 3. Solution Time Comparison\n",
    "            plt.figure()\n",
    "            time_data = pd.melt(results_df, \n",
    "                               id_vars=['capacity'],\n",
    "                               value_vars=['dp_time', 'saa_time'],\n",
    "                               var_name='Algorithm',\n",
    "                               value_name='Time (seconds)')\n",
    "            sns.boxplot(data=time_data, x='capacity', y='Time (seconds)',\n",
    "                       hue='Algorithm')\n",
    "            plt.title('Solution Time Comparison')\n",
    "            plt.xlabel('Capacity Level')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(self.output_dir / 'solution_time_comparison.png')\n",
    "            plt.close()\n",
    "\n",
    "            logger.info(\"Successfully created all visualizations\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating visualizations: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_report(self, results_df: pd.DataFrame, analysis: Dict):\n",
    "        \"\"\"Generate a comprehensive report of experimental results.\"\"\"\n",
    "        try:\n",
    "            report = []\n",
    "            report.append(\"# Experiment 1: Solution Quality Assessment Report\")\n",
    "            report.append(\"\\n## Overview\")\n",
    "            report.append(f\"- Total test instances: {len(results_df)}\")\n",
    "            report.append(f\"- Capacity levels: {sorted(results_df['capacity'].unique())}\")\n",
    "            report.append(f\"- Demand scenarios: {sorted(results_df['demand_scenario'].unique())}\")\n",
    "            report.append(f\"- Market conditions: {sorted(results_df['market_condition'].unique())}\")\n",
    "            report.append(f\"- Replications per configuration: {self.num_replications}\")\n",
    "\n",
    "            report.append(\"\\n## Overall Results\")\n",
    "            report.append(f\"- Mean revenue gap: {analysis['overall']['mean_revenue_gap']:.2f}%\")\n",
    "            report.append(f\"- Revenue gap 95% CI: [{analysis['confidence_intervals']['revenue_gap_lower']:.2f}%, \"\n",
    "                         f\"{analysis['confidence_intervals']['revenue_gap_upper']:.2f}%]\")\n",
    "            report.append(f\"- Mean DP solution time: {analysis['overall']['mean_dp_time']:.2f} seconds\")\n",
    "            report.append(f\"- Mean SAA solution time: {analysis['overall']['mean_saa_time']:.2f} seconds\")\n",
    "\n",
    "            report.append(\"\\n## Statistical Analysis\")\n",
    "            report.append(f\"- T-statistic: {analysis['statistical_tests']['t_statistic']:.4f}\")\n",
    "            report.append(f\"- P-value: {analysis['statistical_tests']['p_value']:.4f}\")\n",
    "\n",
    "            report.append(\"\\n## Results by Capacity Level\")\n",
    "            for capacity in sorted(results_df['capacity'].unique()):\n",
    "                cap_stats = analysis['by_capacity'][str(capacity)]\n",
    "                report.append(f\"\\nCapacity = {capacity}\")\n",
    "                report.append(f\"- Mean revenue gap: {cap_stats['revenue_gap_mean']:.2f}%\")\n",
    "                report.append(f\"- Revenue gap std: {cap_stats['revenue_gap_std']:.2f}%\")\n",
    "                report.append(f\"- Mean DP time: {cap_stats['dp_time_mean']:.2f} seconds\")\n",
    "                report.append(f\"- Mean SAA time: {cap_stats['saa_time_mean']:.2f} seconds\")\n",
    "\n",
    "            # Add summary statistics by demand scenario\n",
    "            report.append(\"\\n## Results by Demand Scenario\")\n",
    "            demand_stats = results_df.groupby('demand_scenario').agg({\n",
    "                'revenue_gap': ['mean', 'std'],\n",
    "                'dp_time': 'mean',\n",
    "                'saa_time': 'mean'\n",
    "            })\n",
    "\n",
    "            for scenario in sorted(results_df['demand_scenario'].unique()):\n",
    "                stats = demand_stats.loc[scenario]\n",
    "                report.append(f\"\\nScenario: {scenario}\")\n",
    "                report.append(f\"- Mean revenue gap: {stats['revenue_gap']['mean']:.2f}%\")\n",
    "                report.append(f\"- Revenue gap std: {stats['revenue_gap']['std']:.2f}%\")\n",
    "                report.append(f\"- Mean DP time: {stats['dp_time']['mean']:.2f} seconds\")\n",
    "                report.append(f\"- Mean SAA time: {stats['saa_time']['mean']:.2f} seconds\")\n",
    "\n",
    "            # Add summary statistics by market condition\n",
    "            report.append(\"\\n## Results by Market Condition\")\n",
    "            market_stats = results_df.groupby('market_condition').agg({\n",
    "                'revenue_gap': ['mean', 'std'],\n",
    "                'dp_time': 'mean',\n",
    "                'saa_time': 'mean'\n",
    "            })\n",
    "\n",
    "            for market in sorted(results_df['market_condition'].unique()):\n",
    "                stats = market_stats.loc[market]\n",
    "                report.append(f\"\\nMarket: {market}\")\n",
    "                report.append(f\"- Mean revenue gap: {stats['revenue_gap']['mean']:.2f}%\")\n",
    "                report.append(f\"- Revenue gap std: {stats['revenue_gap']['std']:.2f}%\")\n",
    "                report.append(f\"- Mean DP time: {stats['dp_time']['mean']:.2f} seconds\")\n",
    "                report.append(f\"- Mean SAA time: {stats['saa_time']['mean']:.2f} seconds\")\n",
    "\n",
    "            # Save report\n",
    "            with open(self.output_dir / 'experiment_report.md', 'w') as f:\n",
    "                f.write('\\n'.join(report))\n",
    "\n",
    "            logger.info(\"Report generated successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating report: {str(e)}\")\n",
    "            logger.error(\"Results DataFrame head:\")\n",
    "            logger.error(results_df.head())\n",
    "            logger.error(\"Analysis structure:\")\n",
    "            logger.error(json.dumps(analysis, indent=2))\n",
    "    \n",
    "    def run_full_experiment(self, num_workers: int = 4):\n",
    "        \"\"\"Execute the complete experiment workflow.\"\"\"\n",
    "        logger.info(\"Starting full experiment execution\")\n",
    "\n",
    "        try:\n",
    "            # Run experiments\n",
    "            results_df = self.run_experiment(num_workers)\n",
    "            logger.info(\"Experiments completed successfully\")\n",
    "\n",
    "            # Analyze results\n",
    "            analysis = self.analyze_results(results_df)\n",
    "            logger.info(\"Analysis completed successfully\")\n",
    "\n",
    "            # Generate report\n",
    "            self.generate_report(results_df, analysis)\n",
    "            logger.info(\"Report generation completed successfully\")\n",
    "\n",
    "            # Create visualizations\n",
    "            self.create_visualizations(results_df)\n",
    "            logger.info(\"Visualization creation completed successfully\")\n",
    "\n",
    "            logger.info(\"Experiment execution completed\")\n",
    "            return results_df, analysis\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in experiment execution: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982ee8e2-971f-411b-a639-94565cbda43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Add process safety for macOS\n",
    "    import multiprocessing\n",
    "    multiprocessing.set_start_method('spawn')\n",
    "    \n",
    "    # Run the complete experiment\n",
    "    experiment = Experiment1Runner()\n",
    "    try:\n",
    "        results, analysis = experiment.run_full_experiment(num_workers=4)\n",
    "        print(\"Experiment completed successfully\")\n",
    "        print(f\"Results saved to: {experiment.output_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error running experiment: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32faeab5-c88e-4daf-995a-a7aaae5029f1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc28556d-4611-45d9-95e8-4b195c2c316a",
   "metadata": {},
   "source": [
    "# Experiment 1: Solution Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3eb596a-f4bb-4d87-843f-c96d73f73431",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:matplotlib.category:Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "INFO:matplotlib.category:Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "INFO:matplotlib.category:Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "INFO:matplotlib.category:Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "INFO:matplotlib.category:Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "INFO:matplotlib.category:Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "INFO:experiment1_solution_quality_assessment_full:Successfully created all visualizations\n"
     ]
    }
   ],
   "source": [
    "from experiment1_solution_quality_assessment_full import Experiment1Runner\n",
    "# Load the saved results\n",
    "results_df = pd.read_csv('results/experiment1/raw_results.csv')\n",
    "\n",
    "# Create an experiment instance\n",
    "experiment = Experiment1Runner()\n",
    "\n",
    "# Generate visualizations\n",
    "experiment.create_visualizations(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90ed358-76d7-4a6b-ba79-442d1600bcbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
